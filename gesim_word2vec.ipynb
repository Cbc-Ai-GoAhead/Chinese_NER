{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0109173-0969-479a-b7e9-985dcfd24a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2ec5521-c51f-4b05-bbfc-73e5e85a02ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m966.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /root/.local/lib/python3.10/site-packages (from gensim) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/lib/python3/dist-packages (from gensim) (1.8.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 KB\u001b[0m \u001b[31m964.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-6.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45fe42bb-7df5-485e-9f43-aa1a5ba46539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy==1.24.4\n",
      "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m176.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.2\n",
      "    Uninstalling numpy-1.26.2:\n",
      "      Successfully uninstalled numpy-1.26.2\n",
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.10 are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed numpy-1.24.4\n"
     ]
    }
   ],
   "source": [
    "!pip install  numpy==1.24.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be5285e7-2d79-4850-998c-4d0a01265297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['凡是'], ['已'], ['yoyoyo', 'you', 'go', 'home', 'now', 'to', 'sleep']]\n"
     ]
    }
   ],
   "source": [
    "# 引入数据集\n",
    "raw_sentences = [\"凡是\", \"已\",\"yoyoyo you go home now to sleep\"]\n",
    "\n",
    "# 切分词汇\n",
    "sentences= [s.split() for s in raw_sentences]\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "037a3473-e398-44df-a58c-d899cb561581",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'similarity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10187/2392555350.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 进行相关性比较\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dogs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'you'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'similarity'"
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "model = word2vec.Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# 进行相关性比较\n",
    "model.similarity('dogs','you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "509e12f3-2a62-490e-ae54-574127bed77a",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "could not find MARK",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10187/2721093933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"sgns.merge.word\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0met_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1951\u001b[0m         \"\"\"\n\u001b[1;32m   1952\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1953\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1954\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m                 \u001b[0mrethrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_lifecycle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \"\"\"\n\u001b[1;32m   1460\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1461\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# needed because loading from S3 doesn't support readline()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: could not find MARK"
     ]
    }
   ],
   "source": [
    "model_path =\"sgns.merge.word\"\n",
    "et_model = gensim.models.Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a4877f16-0fc9-4d3e-9612-a107165976f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dataset = '../Chinese-HealthNER-Corpus/train.json'#這裡我只用train去分成train和test\n",
    "with open(dataset, \"r\", encoding=\"utf-8\") as f:\n",
    "  # train_data = [json.loads(line) for line in fp.read().split(\"\\n\") if line]\n",
    "  train_data = f.read().split(\"\\n\")\n",
    "  # print(train_data[1])\n",
    "train_data = [json.loads(d) for d in train_data if d.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e98dd4ae-911d-4958-bf52-4466b5278edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'且像這類即時心臟超音波診斷，對於瓣膜性病人如二尖瓣和主動脈疾病的人幫助最大，當病人喘不過氣、出現心雜音時，超音波掃瞄一下就能發現是不是瓣膜問題，如果發現瓣膜脫垂嚴重導致血液逆流，趕快通知開刀房和加護病房準備開刀。'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:1][0]['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2eef7b03-3e3c-42bb-ba6b-840897fcae6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['且',\n",
       " '像',\n",
       " '這類',\n",
       " '即時',\n",
       " '心臟超音波',\n",
       " '診斷',\n",
       " '，',\n",
       " '對於',\n",
       " '瓣膜性',\n",
       " '病人',\n",
       " '如',\n",
       " '二尖瓣',\n",
       " '和',\n",
       " '主動脈疾病',\n",
       " '的',\n",
       " '人',\n",
       " '幫助',\n",
       " '最',\n",
       " '大',\n",
       " '，',\n",
       " '當',\n",
       " '病人',\n",
       " '喘不過氣',\n",
       " '、',\n",
       " '出現',\n",
       " '心雜音',\n",
       " '時',\n",
       " '，',\n",
       " '超音波',\n",
       " '掃瞄',\n",
       " '一下',\n",
       " '就',\n",
       " '能',\n",
       " '發現',\n",
       " '是',\n",
       " '不',\n",
       " '是',\n",
       " '瓣膜',\n",
       " '問題',\n",
       " '，',\n",
       " '如果',\n",
       " '發現',\n",
       " '瓣膜',\n",
       " '脫垂',\n",
       " '嚴重',\n",
       " '導致',\n",
       " '血液',\n",
       " '逆流',\n",
       " '，',\n",
       " '趕快',\n",
       " '通知',\n",
       " '開刀房',\n",
       " '和',\n",
       " '加護病房',\n",
       " '準備',\n",
       " '開刀',\n",
       " '。']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:1][0]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5831a87-11db-4a9f-aa91-b6a02bb9033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data = 28161\n"
     ]
    }
   ],
   "source": [
    "print(\"total data = {}\".format(len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca374722-e1ab-42c1-b985-050d6affcee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /root/.local/lib/python3.10/site-packages (from scikit-learn) (1.24.4)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/lib/python3/dist-packages (from scikit-learn) (1.8.0)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.2 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "09751018-ac3d-47c4-b184-304ebb2acc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全部先轉word2vec 要訓練再切分\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, eval_data = train_test_split(train_data, test_size=0.2 ,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "aecd2d1a-61c3-484b-85a0-e87c5ab52017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data len = 22528, eval_data  len = 5633\n"
     ]
    }
   ],
   "source": [
    "print(\"train data len = {}, eval_data  len = {}\".format(len(train_data), len(eval_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "baa9d514-06bf-4e57-be1e-b5a5d59778ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c76e62d5-0c4c-411a-9923-bc2a283a61b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d2a16215-ab7a-4430-8111-b196b27a36ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = \"data/health_train_character.txt\"\n",
    "with open(train_dataset, \"w\", encoding=\"utf-8\") as f:\n",
    "    for id in range(len(train_data)):#len(train_data)\n",
    "        # print( train_data[id])\n",
    "        word_list = train_data[id]['character']#'word'\n",
    "        lable_list =  train_data[id]['character_label']\n",
    "        sentence =\"\"\n",
    "        # print(word_list)\n",
    "        for word, label in zip(word_list, lable_list):\n",
    "            word_label = word +\" \"+label +\"\\n\"\n",
    "            sentence += word_label\n",
    "            # if word == word_list[-1]:\n",
    "            #     sentence +=\" \\n\"\n",
    "            # else:\n",
    "            #     sentence +=\"\\n\"\n",
    "        f.write(sentence)\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dfa95d5d-e51f-44b3-baf6-c6e11124e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = \"data/health_eval_character.txt\"\n",
    "with open(eval_dataset, \"w\", encoding=\"utf-8\") as f:\n",
    "    for id in range(len(eval_data)):#len(train_data)\n",
    "        # print( train_data[id])\n",
    "        word_list = eval_data[id]['character']#'word'\n",
    "        lable_list =  eval_data[id]['character_label']\n",
    "        sentence =\"\"\n",
    "        # print(word_list)\n",
    "        for word, label in zip(word_list, lable_list):\n",
    "            word_label = word +\" \"+label +\"\\n\"\n",
    "            sentence += word_label\n",
    "        f.write(sentence)\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee6be48-7dcf-47fa-a49e-522f53caf42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7463448e-4305-4e51-aec2-a9be25158e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_path = '../Chinese-HealthNER-Corpus/test.json'#這裡我只用train去分成train和test\n",
    "with open(test_dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "  # train_data = [json.loads(line) for line in fp.read().split(\"\\n\") if line]\n",
    "  test_data = f.read().split(\"\\n\")\n",
    "test_data = [json.loads(d) for d in test_data if d.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7716dde4-2224-4553-a56e-aad5a6015149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data len=2531\n"
     ]
    }
   ],
   "source": [
    "print(\"test_data len={}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5ce0dfd0-beaa-4ab5-ae49-d1a6e9866e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "be9fd101-8c5a-4df6-9f26-7524cb22081e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_data[:1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "060e9a8f-92f6-4472-ac29-17358da4991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_truth_path = \"eval/health_test_truth.txt\"\n",
    "with open(test_truth_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for id in range(len(test_data)):#len(train_data)\n",
    "        # print( test_data[id])\n",
    "        # print( type(test_data[id]))\n",
    "        word_list = test_data[id]['character']#'word'\n",
    "        # print(word_list)\n",
    "        lable_list =  test_data[id]['character_label']\n",
    "        sentence =\"\"\n",
    "        # print(word_list)\n",
    "        for word, label in zip(word_list, lable_list):\n",
    "            word_label = word +\" \"+label +\"\\n\"\n",
    "            sentence += word_label\n",
    "        f.write(sentence)\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "00dc7201-63e0-44fd-a68c-75c5e1bb5f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_truth_path = \"data/health_test_character.txt\"\n",
    "with open(test_truth_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for id in range(len(test_data)):#len(train_data)\n",
    "        # print( test_data[id])\n",
    "        # print( type(test_data[id]))\n",
    "        word_list = test_data[id]['character']#'word'\n",
    "        # print(word_list)\n",
    "        # lable_list =  test_data[id]['character_label']\n",
    "        sentence =\"\"\n",
    "        # print(word_list)\n",
    "        for word in word_list:\n",
    "            sentence += word+\"\\n\"\n",
    "            # if word != word_list[-1]:\n",
    "            #     sentence+=\" \"\n",
    "            # else:\n",
    "            #     sentence+=\"\\n\"\n",
    "        f.write(sentence)\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe06dc-978d-4e7b-afcc-1f417624afad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
